<!DOCTYPE html>
<html>
<meta property='og:title' content='HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning'/>
<meta property='og:description' content='HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning'/>
<meta property='og:url' content='https://chuhaozhou99.github.io/ChuhaoZhou.github.io/HoloLLM/'/>
<meta property='og:image:width' content='1200' />
<meta property='og:image:height' content='663' />
<!-- TYPE BELOW IS PROBABLY: 'website' or 'article' or look on https://ogp.me/#types -->
<meta property="og:type" content='website'/>

<head>
  <meta charset="utf-8">
  <meta name="description"
        content="HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning">
  <meta name="keywords" content="HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning</title>
  
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/tab_gallery.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="juxtapose/css/juxtapose.css">
  
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/magnifier.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
  <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/image_card_fader.css">
  <link rel="stylesheet" href="./static/css/image_card_slider.css">
  <link rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Patrick+Hand|Google+Sans|Noto+Sans|Castoro|Lato|Open+Sans&effect=shadow-multiple|emboss|3d"> 
  
  <!-- <link rel="icon" href="./static/images/pyramid.png" type="image/x-icon">
  <link rel="shortcut icon" href="./static/images/pyramid.png" type="image/x-icon"> -->
  
  <!-- The following scripts will take very long loading time-->
  <!--
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  -->
</head>

<!--
<style>
  @import url('https://fonts.cdnfonts.com/css/menlo');

  .video-table td, .video-table th {
    padding-top: 2px;
    padding-bottom: 2px;
    padding-left: 4px;
    padding-right: 4px;
    font-weight: normal;
  }
  .first-col {
    width: 7%;
    vertical-align: middle;
  }
  .other-col {
    width: 31%;
  }
  body {
    font-family: "Lato", sans-serif;
    font-size: 1.1em;
  }
  .title.is-3 {
    font-weight: 900;
    font-size: 2.0rem;
  }
  .title.is-4 {
    font-weight: 700;
    font-size: 1.7rem;
  }
  .custom-emoji {
    width: 1em;
    height: 1em;
    display: inline-block;
    background-image: url('./static/images/pyramid.png');
    background-size: cover;
    vertical-align: middle;
    line-height: 1;
}

</style>
-->

<style>
  .gradient-text-h1 {
    font-size: 40px;
    font-weight: bold;
    background: linear-gradient(to right, #ff6a00, #ff3cac, #4e54c8, #00c6ff);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    color: transparent;
    display: inline;
  }

  .inline-h1 {
    font-size: 40px;
    font-weight: bold;
    display: inline;
  }

  .gradient-text-span {
    font-size: 16px;
    font-weight: bold;
    background: linear-gradient(to right, #ff6a00, #ff3cac, #4e54c8, #00c6ff);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    /* 兼容 Firefox */
    background-clip: text;
    color: transparent;
    display: inline;
  }
</style>
  
<body>
  <section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <br><br>
          <!--<h1 class="title is-2 publication-title" style="font-size: 2.7rem">-->
          <!--<h1 style="font-size: 2.4rem">-->
          <h1 class="gradient-text-h1">
            <!-- modify in index.css/.publication-title to change the color-->
            <!--
            </a><span style="color: #FF4500;"><b>X</b></span><span style="color: #FF6347;"><b>-</b></span><span style="color: #FF8C00;"><b>F</b></span><span style="color: #FF2400;"><b>i</b></span>: A Modality-Invariant Foundation Model for Multimodal Human Sensing
            -->
            <b>
            HoloLLM
            </b>
          </h1>
          <h1 class="inline-h1">
          : Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning
          </h1>
        
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://chuhaozhou99.github.io/ChuhaoZhou.github.io/Chuhao-Zhou" target="_blank">Chuhao Zhou</a>,&nbsp;</span>
            <span class="author-block">
              <a href="https://marsyang.site/" target="_blank">Jianfei Yang</a></span>
          </div>

          <div class="is-size-5 publication-authors">
            <a href="https://marslab.tech"><span class="author-block"><span style="color: #FF4500;">M</span><span style="color: #FF6347;">A</span><span style="color: #FF8C00;">R</span><span style="color: #FF2400;">S</span> <span style="color: #f64125;">L</span><span style="color: #f88000;">a</span><span style="color: #FFA500;">b</span></a>, Nanyang Technological University</span>
          </div>

          <div class="is-size-5 publication-authors">
            {chuhao002@e., jianfei.yang@}ntu.edu.sg
          </div>

          <!-- <div class="is-size-5 publication-venue">
            in XXX
          </div> -->
          
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Code Link. -->
              
              <span class="link-block">
                <a href="https://github.com/ChuhaoZhou99/HoloLLM" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              

              <!-- paper -->
              
              <span class="link-block">
                <a href="https://www.arxiv.org/abs/2505.17645" class="external-link button is-normal is-rounded is-dark" target="_blank">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              

              <!-- doc -->
              <span class="link-block">
                <a href="https://www.arxiv.org/abs/2505.17645"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- twitter -->
              <!-- <span class="link-block">
                <a href="https://x.com/oahzxl/status/1805939975420330298" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-twitter"></i>
                  </span>
                  <span>Twitter</span>
                </a>
              </span> -->
              <!-- bibtex -->
              <span class="link-block">
                <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:J7pK0ojOWV8J:scholar.google.com/&output=citation&scisdr=CgLesAr8ELHd2qXGCWY:AAZF9b8AAAAAaD7AEWZBlWHoSWvCxQEs_JlV9Lw&scisig=AAZF9b8AAAAAaD7AES4Cnp7v1ubZ-GahLkEUCr8&scisf=4&ct=citation&cd=-1&hl=zh-CN"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-obp"></i>
                  </span>
                  <span>BibTex</span>
                </a>
              </span>
            </div>
          </div>
          <br />
          <br />
          <!--
          <div class="is-size-5 publication-authors">
            <span class="author-block"><b>Accepted by ICLR 2025</b></span>
          </div>
          -->
        </div>
      </div>
    </div>
  </div>
</section>



<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="content has-text-justified">
        <p>
          <b>Real-Time Video Generation: Achieved <span style="font-size: 1.3em">🥳</span>!</b> We introduce <span class="custom-emoji"></span><b><span style="color: #ff0000;">P</span><span style="color: #f80007;">y</span><span style="color: #f1000e;">r</span><span style="color: #ea0015;">a</span><span style="color: #e4001c;">m</span><span style="color: #dd0022;">i</span><span style="color: #d60029;">d</span> <span style="color: #cf0030;">A</span><span style="color: #c80037;">t</span><span style="color: #c1003e;">t</span><span style="color: #ba0045;">e</span><span style="color: #b3004c;">n</span><span style="color: #ad0053;">t</span><span style="color: #a60059;">i</span><span style="color: #9f0060;">o</span><span style="color: #980067;">n</span> <span style="color: #91006e;">B</span><span style="color: #8a0075;">r</span><span style="color: #83007c;">o</span><span style="color: #7c0083;">a</span><span style="color: #76008a;">d</span><span style="color: #6f0090;">c</span><span style="color: #680097;">a</span><span style="color: #61009e;">s</span><span style="color: #5a00a5;">t</span></b> (<span style="color: #e2001d;">P</span><span style="color: #a80057;">A</span><span style="color: #6d0092;">B</span>), the first approach that achieves <b>real-time</b> DiT-based video generation. By mitigating redundant attention computation, <span style="color: #e2001d;">P</span><span style="color: #a80057;">A</span><span style="color: #6d0092;">B</span> achieves up to <b>21.6</b> FPS with <b>10.6x</b> acceleration, <b>without sacrificing quality</b> across popular DiT-based video generation models including <a href="https://github.com/hpcaitech/Open-Sora" style="color: blue;">Open-Sora</a>, <a href="https://github.com/PKU-YuanGroup/Open-Sora-Plan" style="color: blue;">Open-Sora-Plan</a>, and <a href="https://github.com/Vchitect/Latte" style="color: blue;">Latte</a>. Notably, as a <b>training-free</b> approach,  <span style="color: #e2001d;">P</span><span style="color: #a80057;">A</span><span style="color: #6d0092;">B</span> can enpower <b>any</b> future DiT-based video generation models with real-time capabilities.
          <br>
        </p>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop">
    <video class="video" autoplay controls muted loop playsinline>
      <source src="./static/videos/compare.mp4" type="video/mp4">
    </video>
    <div class="columns is-centered">
      <div class="content has-text-centered">
        <span style="font-size: 0.8em; width: 80%; display: inline-block;"><br>Video 1: Comparison of video generation speeds between original method and ours. We test on Open-Sora with 5 videos of 4s (96 frames) 480p resolution. Baseline and ours use 1 and 8 NVIDIA H100 GPUs respectively.</span>
        <br>
      </div>
    </div>
  </div>
</section> -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width"> 
        <!-- <h2 class="title is-3">Method</h2> -->
        <!-- <h2 class="title is-4">Modality-Invariant Foundation Model for Human Sensing</h2> -->
        <div class="content has-text-justified">
          <!--
          We introduce <span style="color: #FF4500;"><b>X</b></span><span style="color: #FF6347;"><b>-</b></span><span style="color: #FF8C00;"><b>F</b></span><span style="color: #FF2400;"><b>i</b></span>, the first foundation model that achieves modality-invariant multimodal human sensing. This  model would require <mark>training only once</mark>, allowing <mark>all sensor modalities</mark> that participated in the training process to <mark>be utilized independently or in any combination</mark> for a wide range of potential applications. We evaluated <span style="color: #FF4500;"><b>X</b></span><span style="color: #FF6347;"><b>-</b></span><span style="color: #FF8C00;"><b>F</b></span><span style="color: #FF2400;"><b>i</b></span> on HPE and HAR tasks in MM-Fi <a href="https://openreview.net/pdf?id=1uAsASS1th" target="_blank">[1]</a> and XRF55 <a href="https://dl.acm.org/doi/10.1145/3643543" target="_blank">[2]</a>, demonstrated that <span style="color: #FF4500;"><b>X</b></span><span style="color: #FF6347;"><b>-</b></span><span style="color: #FF8C00;"><b>F</b></span><span style="color: #FF2400;"><b>i</b></span> surpasses previous methods by MPJPE <strong>24.8%</strong> and PA-MPJPE <strong>21.4%</strong> on HPE task, accuracy <strong>2.8%</strong> on HAR task.
          -->
          We introduce <span class="gradient-text-span">HoloLLM</span>, a Multimodal Large Language Model (MLLM) that integrates <strong>uncommon but powerful sensing modalities</strong>, such as LiDAR, infrared, mmWave radar, and WiFi, to enable <strong>seamless</strong> human perception and reasoning across heterogeneous environments. Extensive experiments on two newly constructed benchmarks show that <span class="gradient-text-span">HoloLLM</span> significantly outperforms existing MLLMs, improving language-grounded human sensing accuracy by up to <strong>30%</strong>. This work establishes a new foundation for real-world, language-informed <strong>multisensory embodied intelligence</strong>.
          <div class="content has-text-centered">
            <br>
            <img src="./static/images/holollm_teaser.png"><br>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> 

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Embodied agents operating in smart homes must understand human behavior through diverse sensory inputs and communicate via natural language. While Vision-Language Models (VLMs) have enabled impressive language-grounded perception, their reliance on visual data limits robustness in real-world scenarios with occlusions, poor lighting, or privacy constraints. In this paper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that integrates uncommon but powerful sensing modalities, such as LiDAR, infrared, mmWave radar, and WiFi, to enable seamless human perception and reasoning across heterogeneous environments. We address two key challenges: (1) the scarcity of aligned modality-text data for rare sensors, and (2) the heterogeneity of their physical signal representations. To overcome these, we design a Universal Modality-Injection Projector (UMIP) that enhances pre-aligned modality embeddings with fine-grained, text-aligned features from tailored encoders via coarse-to-fine cross-attention without introducing significant alignment overhead. We further introduce a human-VLM collaborative data curation pipeline to generate paired textual annotations for sensing datasets. Extensive experiments on two newly constructed benchmarks show that HoloLLM significantly outperforms existing MLLMs, improving language-grounded human sensing accuracy by up to 30%. This work establishes a new foundation for real-world, language-informed multisensory embodied intelligence.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">        
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            Embodied agents in smart homes, e.g., household robots and intelligent appliances, have garnered increasing attention in recent years. To interact effectively with humans and execute real-world tasks, agents must understand human behavior and be capable of engaging in natural language communication. This necessitates the development of models that seamlessly integrate rich human perception with advanced language understanding and generation capabilities.
            <br>
            Vision-Language Models (VLMs) have emerged as promising tools for enabling language-conditioned perception and reasoning. However, the visual modality alone struggles to operate in the real world, e.g., low-light environments, occlusions, and privacy-sensitive scenarios.
            <br>
            In contrast, humans naturally rely on multiple sensory modalities, such as vision, audition, and olfaction, to perceive and adapt to diverse environments. Similarly, sensing modalities brings distinct advantages: LiDAR enables high-precision 3D reconstruction, infrared cameras support perception in darkness, and mmWave radar and WiFi are resilient to visual occlusions and lighting variations. Hence, we propose <span class="gradient-text-span">HoloLLM</span> that integrates diverse sensor inputs to provide excellent adaptability and reliability in complex, real-world environments.
          </p>
          <!--
          <div class="container is-max-desktop">
            <video class="video" autoplay controls muted loop playsinline>
              <source src="./static/videos/concept.mp4" type="video/mp4">
            </video>
          </div>
          -->
        </div>
<!-- 
        <h2 class="title is-4">Parallelism</h2>
        <div class="content has-text-centered">
          <div class="content has-text-centered">
            <img src="./static/images/parallel.png" style="width: 70%;"><br>
            <span style="font-size: 0.8em; width: 80%; display: inline-block;">Figure 3: Comparison between original Dynamic Sequence Parallel (DSP) and ours. When temporal attention is broadcasted, we can avoid all communication.</span>
          </div>
          <div class="content has-text-justified">
            To further enhance video generation speed, we improve sequence parallel based on <a href="https://arxiv.org/abs/2403.10266" target="_blank">Dynamic Sequence Parallelism</a> (DSP). Sequence parallelism segments videos into different parts across multiple GPUs, reducing the workload on each GPU and decreasing generation latency. However, DSP introduces significant communication overhead, requiring two all-to-all communications for temporal attention. By broadcasting temporal attention in <span style="color: #e2001d;">P</span><span style="color: #a80057;">A</span><span style="color: #6d0092;">B</span>, we eliminate these communications, as temporal attention no longer needs to be computed. This results in a significant reduction in communication overhead by over 50%, enabling more efficient distributed inference for real-time video generation.
          </div>
        </div> -->
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width"> 
        <h2 class="title is-3">Method</h2>
        <!-- <h2 class="title is-4">Modality-Invariant Foundation Model for Human Sensing</h2> -->
        <div class="content has-text-justified">
          <p>
            We propose a novel multisensory foundation model, <span class="gradient-text-span">HoloLLM</span>, for seamless human perception and reasoning across heterogeneous environments.
          </p>
          <p>
            <span class="gradient-text-span">HoloLLM</span> takes the Universal Modality-Injection Projector (UMIP) to efficiently align sensing modalities with the text via only minimal fine-tuning. Besides, the modality-specific discriminative features are adequately explored by tailored encoders and adaptively injected into the aligned multimodal tokens through UMIP.
          </p>
          <div class="content has-text-centered">
            <img src="./static/images/holollm_pipeline.png"><br>
            <span style="font-size: 0.8em; width: 100%; display: inline-block;">Architecture of HoloLLM. Given multimodal inputs \( X^m \), HoloLLM utilizes modality-specific tokenizers and a universal encoder to extract pre-aligned initial embeddings \( Y^m_{CLIP} \). Meanwhile, pre-trained tailored encoders are applied to explore modality features \( Y^m_{T} \). The UMIP then transforms \( Y^m_{CLIP} \) and \( Y^m_{T} \) into coarse queries \( Q^m \) and fine-grained keys and values \( K^m \) / \( V^m \). By iteratively enhancing the queries via coarse-to-fine cross-attention and projecting them to the LLM text space, the aligned multimodal tokens \( Z^m \) fully enriched by modality features can be achieved.
          </div>
          <p>
            We compare UMIP with state-of-the-art multimodal projectors. Specifically, most existing works adopt modality-specific encoders and projectors, which commonly requires substantial ‘modality-text’ data pairs for pre-training. OneLLM <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Han_OneLLM_One_Framework_to_Align_All_Modalities_with_Language_CVPR_2024_paper.html">[1]</a> attempts to handle various modalities via a unified framework that consists of a universal encoder and projector. However, without a dedicated design for capturing heterogeneous spatial features, the universal encoder struggles to obtain sufficiently discriminative multimodal tokens. Different from existing works, UMIP only utilizes the universal encoder to generate initial embeddings for each modality. These embeddings are then progressively enhanced by fine-grained, text-aligned features from tailored encoders.
          </p>
          <div class="content has-text-centered">
            <img src="./static/images/holollm_comparison.png"><br>
            <span style="font-size: 0.8em; width: 100%; display: inline-block;">Comparison between UMIP and other projectors: (a) Modality-Specific Projector, (b) Universal Projector, and (c) Universal Modality-Injection Projector (Ours).
          </div>  
        </div>
      </div>
    </div>
  </div>
</section> 


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Evaluations</h2>
        <h2 class="title is-4">Qualitative Results</h2>
        <div class="container is-max-desktop">
          <h2 class="title is-5">Visualization of Multimodal and Text Tokens</h2>
          <div class="content has-text-justified">
            <img src="./static/images/holollm_tsne.png"><br>
            <br>
            <p>
            Visualization results by tSNE. (a) Visualization of aligned tokens from 5 action
            categories (denoted by different colors) generated by Baseline, OneLLM, and HoloLLM for
            ‘Video’ and ‘mmWave’ modalities. (b) Visualization of multimodal tokens from 2 action categories
            (denoted by different colors) for ‘mmWave’ (circles), ‘WiFi’ (pentagrams), and ‘Text’ (triangles)
            modalities generated by HoloLLM without or with UMIP.
            </p>
          </div>
        </div>
        <br>
        <div class="container is-max-desktop">
          <h2 class="title is-5">
            Qualitative Results across Common and Sensing Modalities
          </h2>
          <div class="content has-text-justified">
            <img src="./static/images/holollm_qualitative.png"><br>
            <br>
            <p>
              We give some qualitative results of HoloLLM in the “CrossEnv” setting. These results show that HoloLLM can perform Action QA and Action Caption tasks across common and sensing modalities in diverse environments.
            </p>
          </div>
          <!-- defined the interpolated image path in index.js function -->
        </div>

        
        <!-- <h2 class="title is-4">Speedups</h2>
        <div class="content has-text-centered">
          <img src="./static/images/speedup.png">
        </div>
        <div class="content has-text-justified">
          <p>
            Measured total latency of <span style="color: #e2001d;">P</span><span style="color: #a80057;">A</span><span style="color: #6d0092;">B</span> for different models for generating a single video on 8 NVIDIA H100 GPUs. When utilizing a single GPU, we achieve a speedup ranging from 1.26x to 1.32x, which remains stable across different schedulers. Scaling to multiple GPUs, our method achieves a speedup of up to 10.6x, which almost linearly scales with the number of GPUs due to our efficient improvement of sequence parallel.
          </p> -->
        </div>
      </div>
    </div>
  </div>
  <br>

  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">        
        <div class="content has-text-justified">
          <h2 class="title is-4">Quantitive Results</h2>
          <p>
            We train and evaluate our proposed <span class="gradient-text-span">HoloLLM</span> on two multimodal human-sensing datasets MM-Fi <a href="https://openreview.net/pdf?id=1uAsASS1th" target="_blank">[2]</a> and XRF55 <a href="https://dl.acm.org/doi/10.1145/3643543" target="_blank">[3]</a> with generated textual desriptions. Specifically, MM-Fi consists of 5 modalities: Video (V), Depth images (D), LiDAR (L), mmWave Radar (M), and WiFi-CSI (W). Besides, XRF55 also contains 5 modalities: Video (V), Depth images (D), Infrared images (I), RFID signals (R), and WiFi-CSI (W).
            <br>
            <br>
            To comprehensively evaluate various MLLMs across diverse scenarios, we design three experimental settings: (1) Random Split (Random), (2) Cross-Subject Split (CrossSub), and (3) Cross-Environment Split (CrossEnv). Specifically, ‘Random’ involves a random split of all samples with a ratio of 3:1, and ‘CrossSub’ / ‘CrossEnv’ selects samples from nonoverlapping human subjects / environments for training and testing.
            <br>
            <br>
            For quantitative evaluation, we use the accuracy for Action Recognition and Action QA, and the METEOR metric for Action Caption.
          </p>
          <div class="title is-5">Action QA and Caption Results on MM-Fi</div>
          <div class="content has-text-centered">
            <img src="./static/images/holollm_mmfi.png" style="width: 100%;"><br>
            <span style="font-size: 0.8em; width: 100%; display: inline-block;">Evaluation of Human Action QA and Caption tasks on MM-Fi across three settings. The Accuracy (%) and METEOR (%) are adopted for Action QA and Caption. </span>
          </div>
          <div class="title is-5">Action QA and Caption Results on XRF55</div>
          <div class="content has-text-centered">
            <img src="./static/images/holollm_xrf55.png" style="width: 100%;"><br>
            <span style="font-size: 0.8em; width: 100%; display: inline-block;">Evaluation of Human Action QA and Caption tasks on XRF55 across three settings. The Accuracy (%) and METEOR (%) are adopted for Action QA and Caption. </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related works</h2>
        <div class="content has-text-justified">
          <p>
            <li>
              Jianfei Yang, He Huang, Yunjiao Zhou, Xinyan Chen, Yuecong Xu, Shenghai Yuan, Han Zou, Chris Xiaoxuan Lu, and Lihua Xie. <a href="https://openreview.net/pdf?id=1uAsASS1th">Mm-fi: Multi-modal non-intrusive 4d human dataset for versatile wireless sensing</a>. Advances in Neural Information Processing Systems, 36, 2024.
            </li>
            <li>
              Fei Wang, Yizhe Lv, Mengdie Zhu, Han Ding, and Jinsong Han. <a href="https://dl.acm.org/doi/10.1145/3643543">Xrf55: A radio frequency dataset for human indoor action analysis</a>. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 8(1):1–34, 2024.
            </li>
          </p>
        </div>

        <!-- <h2 class="title is-3">Acknowledgments</h2>
        <div class="content has-text-justified">
          <p>
            Xuanlei, Xiaolong, and Kai contribute equally to this work. Kai and Yang are equal advising.
          </p>
        </div> -->
      </div>
    </div>
  </div>

</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title"><a id="bibtex">BibTeX</a></h2>
    <pre><code>@misc{zhou2025holollm,
      title={HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning}, 
      author={Chuhao Zhou and Jianfei Yang},
      year={2025},
      eprint={2505.17645},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://www.arxiv.org/abs/2505.17645}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a href="https://nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script src="juxtapose/js/juxtapose.js"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>    

</body>
</html>
